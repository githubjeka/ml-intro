{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification. Linear models and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementing Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you need to implement Logistic Regression with l2 regularization using gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression loss:\n",
    "$$ L(w) = \\dfrac{1}{N}\\sum_{i=1}^N \\log(1 + e^{-\\langle w, x_i \\rangle y_i}) + \\frac{1}{2C} \\lVert w \\rVert^2  \\to \\min_w$$\n",
    "$$\\langle w, x_i \\rangle = \\sum_{j=1}^n w_{j}x_{ij} + w_{0},$$ $$ y_{i} \\in \\{-1, 1\\}$$ where $n$ is the number of features and $N$ is the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent step:\n",
    "$$w^{(t+1)} := w^{(t)} + \\dfrac{\\eta}{N}\\sum_{i=1}^N y_ix_i \\Big(1 - \\dfrac{1}{1 + exp(-\\langle w^{(t)}, x_i \\rangle y_i)}\\Big) - \\eta \\frac{1}{C} w,$$\n",
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2 points)** Implement the algorithm and use it to classify the digits (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) into \"even\" and \"odd\" categories. \"Even\" and \"Odd\" classes  should correspond to {-1, 1} labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopping criteria: either the number of iterations exceeds *max_iter* or $||w^{(t+1)} - w^{(t)}||_2 < tol$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import NotFittedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomLogisticRegression:\n",
    "    _estimator_type = \"classifier\"\n",
    "\n",
    "    def __init__(self, eta=0.001, max_iter=1000, C=1.0, tol=1e-5, random_state=42, zero_init=False):\n",
    "        \"\"\"Logistic Regression classifier.\n",
    "\n",
    "        Args:\n",
    "            eta: float, default=0.001\n",
    "                Learning rate.\n",
    "            max_iter: int, default=1000\n",
    "                Maximum number of iterations taken for the solvers to converge.\n",
    "            C: float, default=1.0\n",
    "                Inverse of regularization strength; must be a positive float.\n",
    "                Smaller values specify stronger regularization.\n",
    "            tol: float, default=1e-5\n",
    "                Tolerance for stopping criteria.\n",
    "            random_state: int, default=42\n",
    "                Random state.\n",
    "            zero_init: bool, default=False\n",
    "                Zero weight initialization.\n",
    "        \"\"\"\n",
    "        self.eta = eta\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.tol = tol\n",
    "        self.random_state = np.random.RandomState(seed=random_state)\n",
    "        self.zero_init = zero_init\n",
    "\n",
    "    def get_sigmoid(self, X, weights):\n",
    "        \"\"\"Compute the sigmoid value.\"\"\"\n",
    "\n",
    "        # linear combinations of weights and sample features\n",
    "        # y = w_0 + w_1*x_1 + ... + w_n*x_n\n",
    "\n",
    "        result = []\n",
    "        for x in X:\n",
    "            wx = np.sum(weights @ x)\n",
    "            result.append(1 / (1 + np.exp(-wx)))\n",
    "        return result\n",
    "\n",
    "    def get_loss(self, x, weights, y):\n",
    "        \"\"\"Calculate the loss.\"\"\"\n",
    "        N_samples = x.shape[0]\n",
    "        n_features = x.shape[1]\n",
    "\n",
    "        sum = 0\n",
    "        for i in range(N_samples):\n",
    "            wx = np.zeros(n_features)\n",
    "            for j in range(1, n_features):\n",
    "                wx += weights[j] * x[i][j]\n",
    "            wx = np.sum(wx) + weights[0]\n",
    "            sum += np.log(1 + np.exp(-(wx * y[i])))\n",
    "        return sum / N_samples + np.linalg.norm(weights) / (2 * self.C)\n",
    "        # return sum / N_samples + 1 / (2 * self.C) * (np.sum(weights) ** 2)\n",
    "\n",
    "        # count = len(x)\n",
    "        # l2norm = np.sum(weights ** 2) ** 0.5\n",
    "        # wx = np.sum(x * weights[1:] + weights[0])\n",
    "        # sum_log = np.sum(np.log(1 + np.exp(-1 * wx * y)))\n",
    "        # reg_strength = 1 / (2 * self.C)\n",
    "        # return sum_log / count + reg_strength * l2norm\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model.\n",
    "\n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Target vector.\n",
    "        \"\"\"\n",
    "        X_ext = np.hstack([np.ones((X.shape[0], 1)), X])  # a constant feature is included to handle intercept\n",
    "        num_features = X_ext.shape[1]\n",
    "        if self.zero_init:\n",
    "            self.weights_ = np.zeros(num_features)\n",
    "        else:\n",
    "            weight_threshold = 1.0 / (2 * num_features)\n",
    "            self.weights_ = self.random_state.uniform(low=-weight_threshold,\n",
    "                                                      high=weight_threshold,\n",
    "                                                      size=num_features)  # random weight initialization\n",
    "        N = X_ext.shape[0]\n",
    "        self.loss_history = [self.get_loss(X, self.weights_, y)]\n",
    "        for i in range(self.max_iter):\n",
    "            delta_grad = np.zeros(num_features)\n",
    "            for j in range(N):\n",
    "                delta_grad += X_ext[j] * y[j] * (1 - 1 / (1 + np.exp(-(self.weights_ @ X_ext[j] * y[j]))))\n",
    "            delta = self.weights_ / self.C - delta_grad / N\n",
    "            self.weights_ -= self.eta * delta\n",
    "            self.loss_history.append(self.get_loss(X, self.weights_, y))\n",
    "            # wx = np.sum(X @ self.weights_[1:]) + self.weights_[0]\n",
    "            # # wx = np.sum(X_ext @ self.weights_)\n",
    "            # exp = 1 - 1 / (1 + np.exp(-(self.weights_ @ X_ext.T @ y)))\n",
    "            # exp = 1 - 1 / (1 + np.exp(-(np.sum(y*wx))))\n",
    "            # delta = (y @ X_ext) * exp / len(y) - self.weights_ / self.C\n",
    "            # self.weights_ += self.eta * delta\n",
    "            # w(t+1)-w(t) = self.eta * delta\n",
    "            if np.linalg.norm(self.eta * delta) < self.tol:\n",
    "                break\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict positive class probabilities.\n",
    "\n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Vector containing positive class probabilities.\n",
    "        \"\"\"\n",
    "        X_ext = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        if hasattr(self, 'weights_'):\n",
    "            return self.get_sigmoid(X_ext, self.weights_)\n",
    "        else:\n",
    "            raise NotFittedError(\"CustomLogisticRegression instance is not fitted yet\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes.\n",
    "\n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Vector containing predicted class labels.\n",
    "        \"\"\"\n",
    "        return [1 if y >= 0.5 else -1 for y in self.predict_proba(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(list(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_digits(n_class=10, return_X_y=True)\n",
    "\n",
    "_, axes = plt.subplots(nrows=3, ncols=7, figsize=(10, 5))\n",
    "for ax, image, label in zip(axes.flatten(), X, y):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image.reshape((8, 8)), cmap=plt.cm.gray_r if label % 2 else plt.cm.afmhot_r)\n",
    "    ax.set_title(label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "#y_train = \"<your code>\"\n",
    "#y_test = \"<your code>\"\n",
    "y_train = (y_train % 2) * 2 - 1\n",
    "y_test = (y_test % 2) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (np.unique(y_train) == [-1, 1]).all()\n",
    "assert (np.unique(y_test) == [-1, 1]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate(clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    disp = metrics.plot_confusion_matrix(clf, X_test, y_test, normalize='true')\n",
    "    disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    acc_train = metrics.accuracy_score(y_pred=clf.predict(X_train), y_true=y_train)\n",
    "    acc_test = metrics.accuracy_score(y_pred=clf.predict(X_test), y_true=y_test)\n",
    "\n",
    "    return acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = CustomLogisticRegression(max_iter=1, zero_init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(lr_clf.get_sigmoid(np.array([[0.5, 0, 1.0], [0.3, 1.3, 1.0]]), np.array([0.5, -0.5, 0.1])),\n",
    "                   np.array([0.58662, 0.40131]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(lr_clf.weights_, np.array([3.1000e-06, 0.0000e+00, 4.1800e-05, 5.4770e-04, 2.2130e-04,\n",
    "                                              4.8750e-04, 1.3577e-03, 5.9780e-04, 5.6400e-05, -7.0000e-07,\n",
    "                                              1.6910e-04, 2.5190e-04, -4.3700e-04, 3.6190e-04, 1.0049e-03,\n",
    "                                              4.2280e-04, 2.5700e-05, 3.0000e-07, -1.1500e-05, -7.2440e-04,\n",
    "                                              -2.6200e-04, 8.7540e-04, 4.1540e-04, -8.4200e-05, -5.2000e-06,\n",
    "                                              0.0000e+00, -2.2160e-04, -5.7130e-04, 9.8570e-04, 1.3507e-03,\n",
    "                                              5.0210e-04, -1.7050e-04, -1.0000e-06, 0.0000e+00, -6.7810e-04,\n",
    "                                              -1.0515e-03, -4.4500e-05, 3.7160e-04, 4.2100e-04, -8.1800e-05,\n",
    "                                              0.0000e+00, -5.2000e-06, -5.3410e-04, -2.0393e-03, -8.4310e-04,\n",
    "                                              1.0400e-04, -1.2390e-04, -1.7880e-04, -1.3200e-05, -4.5000e-06,\n",
    "                                              -9.4300e-05, -1.1127e-03, -5.0900e-04, -2.1850e-04, -5.6050e-04,\n",
    "                                              -3.9560e-04, -1.7700e-05, -3.0000e-07, 2.6800e-05, 6.3920e-04,\n",
    "                                              1.8090e-04, -7.3660e-04, -5.3930e-04, -3.7060e-04, -2.8200e-05]),\n",
    "                   atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, test_acc = fit_evaluate(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert min(train_acc, test_acc) > 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Visualize the loss history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## your code\n",
    "model = CustomLogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "sns.lineplot(data=model.loss_history)\n",
    "plt.xlabel('кол. итераций')\n",
    "plt.ylabel('logloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Try different learning rates and compare the results. How does the learning rate influence the convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_variants = [0.002, 0.001, 0.000_1, 0.000_01, 0.000_001]\n",
    "for eta in eta_variants:\n",
    "    model = CustomLogisticRegression(eta=eta, max_iter=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    sns.lineplot(data=model.loss_history)\n",
    "    plt.legend(eta_variants)\n",
    "    plt.xlabel('кол. итераций')\n",
    "    plt.ylabel('logloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С уменьшением $\\eta$ (learning rate) требуется больше количество итераций для того, чтобы достичь наименьшего значения logloss. При $\\eta= 0.000001$ мы упираемся в параметр tol (Tolerance for stopping criteria)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Try different regularization parameter values and compare the model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_variants = [1, 0.5, 0.1]\n",
    "for C in c_variants:\n",
    "    model = CustomLogisticRegression(C=C, max_iter=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    sns.lineplot(data=model.loss_history)\n",
    "    plt.legend(c_variants)\n",
    "    plt.xlabel('кол. итераций')\n",
    "    plt.ylabel('logloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C: float, default=1.0\n",
    "Inverse of regularization strength; must be a positive float.\n",
    "Smaller values specify stronger regularization.\n",
    "\n",
    "Часть формулы\n",
    "\n",
    "$\\eta \\frac{1}{C} w,$\n",
    "\n",
    "С увеличением силы регуляризация $C$ максимальное значение logloss уменьшается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Compare zero initialization and random initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_variants = [False, True]\n",
    "for zero_init_value in zero_variants:\n",
    "    model = CustomLogisticRegression(zero_init=zero_init_value, max_iter=40)\n",
    "    model.fit(X_train, y_train)\n",
    "    acc_train = metrics.accuracy_score(y_pred=model.predict(X_train), y_true=y_train)\n",
    "    acc_test = metrics.accuracy_score(y_pred=model.predict(X_test), y_true=y_test)\n",
    "    precision_test = metrics.precision_score(y_pred=model.predict(X_test), y_true=y_test)\n",
    "    recall_test = metrics.recall_score(y_pred=model.predict(X_test), y_true=y_test)\n",
    "    print(zero_init_value, acc_train, acc_test, precision_test, recall_test)\n",
    "\n",
    "    sns.lineplot(data=model.loss_history)\n",
    "    plt.legend(zero_variants)\n",
    "    plt.xlabel('кол. итераций')\n",
    "    plt.ylabel('logloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero_init: bool, default=False Zero weight initialization.\n",
    "При обнулённых инициализированных весах, logloss логарифмически растёт с единицы, при увеличении итераций. При случайном наборе начальных весов, можно увидеть оптимальное количество итераций при которых logloss минимален."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you need to implement weighted K-Neighbors Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that training a KNN classifier is simply memorizing a training sample. \n",
    "\n",
    "The process of applying a classifier for one object is to find the distances from it to all objects in the training data, then select the k nearest objects (neighbors) and return the most common class among these objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also give the nearest neighbors weights in accordance with the distance of the object to them. In the simplest case (as in your assignment), you can set the weights inversely proportional to that distance. \n",
    "\n",
    "$$w_{i} = \\frac{1}{d_{i} + eps},$$\n",
    "\n",
    "where $d_{i}$ is the distance between object and i-th nearest neighbor and $eps$ is the small value to prevent division by zero.\n",
    "\n",
    "In case of 'uniform' weights, all k nearest neighbors are equivalent (have equal weight, for example $w_{i} = 1, \\forall i \\in(1,k)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the probability of classes, it is necessary to normalize the weights of each class, dividing them by the sum:\n",
    "\n",
    "$$p_{i} = \\frac{w_{i}}{\\sum_{j=1}^{c}w_{j}},$$\n",
    "\n",
    "where $p_i$ is probability of i-th class and $c$ is the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2 points)** Implement the algorithm and use it to classify the digits. By implementing this algorithm, you will be able to classify numbers not only into \"even\" or \"odd\", but into their real representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomKNeighborsClassifier:\n",
    "    _estimator_type = \"classifier\"\n",
    "\n",
    "    def __init__(self, n_neighbors=5, weights='uniform', eps=1e-9):\n",
    "        \"\"\"K-Nearest Neighbors classifier.\n",
    "        \n",
    "        Args:\n",
    "            n_neighbors: int, default=5\n",
    "                Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
    "            weights : {'uniform', 'distance'} or callable, default='uniform'\n",
    "                Weight function used in prediction.  Possible values:\n",
    "                - 'uniform' : uniform weights.  All points in each neighborhood\n",
    "                  are weighted equally.\n",
    "                - 'distance' : weight points by the inverse of their distance.\n",
    "                  in this case, closer neighbors of a query point will have a\n",
    "                  greater influence than neighbors which are further away.\n",
    "            eps : float, default=1e-5\n",
    "                Epsilon to prevent division by 0 \n",
    "        \"\"\"\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "        self.eps = eps\n",
    "\n",
    "    def get_pairwise_distances(self, X, Y):\n",
    "        \"\"\"\n",
    "        Returnes matrix of the pairwise distances between the rows from both X and Y.\n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "            Y: numpy array of shape (k_samples, n_features)\n",
    "        Returns:\n",
    "            P: numpy array of shape (n_samples, k_samples)\n",
    "                Matrix in which (i, j) value is the distance \n",
    "                between i'th row from the X and j'th row from the Y.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        k_samples = Y.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "        if n_features != Y.shape[1]:\n",
    "            raise Exception('Количество характеристик разное')\n",
    "\n",
    "        p = []\n",
    "        for n_sample in range(n_samples):\n",
    "            p_row_y_for_x = []\n",
    "            for k_sample in range(k_samples):\n",
    "                d = np.linalg.norm(X[n_sample] - Y[k_sample])\n",
    "                p_row_y_for_x.append(d)\n",
    "            p.append(p_row_y_for_x)\n",
    "\n",
    "        return np.array(p)\n",
    "\n",
    "    def get_class_weights(self, y, weights):\n",
    "        \"\"\"\n",
    "        Returns a vector with sum of weights for each class \n",
    "        Args:\n",
    "            y: numpy array of shape (n_samles,)\n",
    "            weights: numpy array of shape (n_samples,)\n",
    "                The weights of the corresponding points of y.\n",
    "        Returns:\n",
    "            p: numpy array of shape (n_classes)\n",
    "                Array where the value at the i-th position \n",
    "                corresponds to the weight of the i-th class.\n",
    "        \"\"\"\n",
    "        p = np.zeros(np.shape(model.classes_))\n",
    "        classes = np.array(model.classes_)\n",
    "\n",
    "        for y_index, class_name in enumerate(y):\n",
    "            # np.add.at(p, y[y_index], weights[y_index])\n",
    "            p[np.where(classes == class_name)[0][0]] += weights[y_index]\n",
    "        return p\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Target vector.        \n",
    "        \"\"\"\n",
    "        self.points = X\n",
    "        self.y = y\n",
    "        self.classes_ = np.unique(y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict positive class probabilities.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples, n_classes)\n",
    "                Vector containing positive class probabilities.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'points'):\n",
    "            distance = self.get_pairwise_distances(X, self.points)\n",
    "\n",
    "            weights_of_points = np.ones(len(distance))\n",
    "            if self.weights == 'distance':\n",
    "                weights_of_points = 1 / (distance + self.eps)\n",
    "\n",
    "            # norm_w_classes = self.get_class_weights(self.y, weights_of_points)\n",
    "\n",
    "            n_samples = X.shape[0]\n",
    "            n_classes = np.shape(model.classes_)[0]\n",
    "\n",
    "            # print(norm_w_classes.shape)\n",
    "\n",
    "            # norm_w_classes = self.get_class_weights(self.y, weights_of_points)\n",
    "\n",
    "            # all_rows = np.arange(n_samples)\n",
    "            # print(distance)\n",
    "            # probabilities = []\n",
    "            # _y = self.y.reshape((-1, 1))\n",
    "            # for k, classes_k in enumerate(model.classes_):\n",
    "            #     pred_labels = _y[:][self.n_neighbors]\n",
    "            #     proba_k = np.zeros((n_samples, n_classes))\n",
    "            #\n",
    "            #     # a simple ':' index doesn't work right\n",
    "            #     for i, idx in enumerate(pred_labels.T):  # loop is O(n_neighbors)\n",
    "            #         proba_k[all_rows, idx] += weights_of_points[:, i]\n",
    "            #\n",
    "            #     # normalize 'votes' into real [0,1] probabilities\n",
    "            #     normalizer = proba_k.sum(axis=1)[:, np.newaxis]\n",
    "            #     normalizer[normalizer == 0.0] = 1.0\n",
    "            #     proba_k /= normalizer\n",
    "            #\n",
    "            #     probabilities.append(proba_k)\n",
    "            #\n",
    "            # return probabilities\n",
    "\n",
    "            # print(pd.DataFrame(weights_of_points))\n",
    "            # probabilities = []\n",
    "            #\n",
    "            # # n_classes = np.shape(model.classes_)[0]\n",
    "            # # # sum_w_by_class = self.get_class_weights(self.y, self.weights)\n",
    "            # # for index_class in range(n_samples) :\n",
    "            # #     probabilities_n = np.zeros(n_classes)\n",
    "            # #     # probability = w / sum_w_by_class\n",
    "            # #     # probability = w\n",
    "            # #     probabilities.append(probabilities_n)\n",
    "            # #\n",
    "            # # return np.array(probabilities)\n",
    "\n",
    "        else:\n",
    "            raise NotFittedError(\"CustomKNeighborsClassifier instance is not fitted yet\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Vector containing predicted class labels.\n",
    "        \"\"\"\n",
    "        # <your code>\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = CustomKNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "X, y = datasets.load_digits(n_class=10, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "knn.fit(X_train, list(map(str, y_train)))\n",
    "model.predict_proba(X_test)\n",
    "# print(model.predict_proba(X_test), knn.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomKNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(model.get_pairwise_distances(np.array([[0, 1], [1, 1]]),\n",
    "                                                np.array([[0.5, 0.5], [1, 0]])),\n",
    "                   np.array([[0.70710678, 1.41421356],\n",
    "                             [0.70710678, 1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_ = ['one', 'two', 'three']\n",
    "assert np.allclose(model.get_class_weights(np.array(['one', 'one', 'three', 'two']), np.array([1, 1, 0, 4])),\n",
    "                   np.array([2, 4, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_digits(n_class=10, return_X_y=True)\n",
    "\n",
    "_, axes = plt.subplots(nrows=3, ncols=7, figsize=(10, 5))\n",
    "for ax, image, label in zip(axes.flatten(), X, y):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image.reshape((8, 8)), cmap=plt.cm.gray_r if label % 2 else plt.cm.afmhot_r)\n",
    "    ax.set_title(label)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "knn.fit(X_train, list(map(str, y_train)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(model.predict_proba(X_test), knn.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, test_acc = fit_evaluate(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_acc == 1\n",
    "assert test_acc > 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Take a look at the confusion matrix and tell what numbers the model confuses and why this happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_acc, test_acc = fit_evaluate(knn, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно наблюдать, что проблемы с определением значения числа возникает на цифрах 5, 7, 9.\n",
    "- 9 похожа на 4 и 5\n",
    "- 7 похожа на 9\n",
    "- 5 похожа на 6 и 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Try different n_neighbors parameters and compare the output probabilities of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [5, 10, 50, 100]\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "    print('При значении K =', k)\n",
    "    train_acc, test_acc = fit_evaluate(knn, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С увеличением параметра K появляются сложности с определением значений других цифр, которые вызывали трудности в прогнозе при k=5. И всё также наблюдаются проблемы у цифр 9, 7, 5, а при увеличении K шанс ошибиться с этими цифрами немного растёт."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Compare both 'uniform' and 'distance' weights and share your thoughts in what situations which parameter can be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "print('weights = uniform')\n",
    "train_acc, test_acc = fit_evaluate(knn, X_train, y_train, X_test, y_test)\n",
    "print(train_acc, test_acc)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "print('weights = distance')\n",
    "train_acc, test_acc = fit_evaluate(knn, X_train, y_train, X_test, y_test)\n",
    "print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При weights = distance метрика Accuracy на тренировочных данных показывает идеальный результат, 100% угадывание, а при weights = uniform, метрика Accuracy на тренировочных данных показывает 99%\n",
    "Значение weights = distance предпочтительнее, так как связи с соседями имеют веса, что по своей сути улучшает прогноз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Suggest another distance measurement function that could improve the quality of the classification for this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данный момент мы использовали функцию Эвклидового расстояния. Евклидово расстояние прекрасно работает, когда у вас маломерные данные, и величина векторов важна для измерения.\n",
    "Но существуют и [другие функции](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html#sklearn.metrics.DistanceMetric):\n",
    "ManhattanDistance\n",
    "ChebyshevDistance\n",
    "MinkowskiDistance\n",
    "WMinkowskiDistance\n",
    "SEuclideanDistance\n",
    "MahalanobisDistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Suggest different task and distance function that you think would be suitable for it.\n",
    "\n",
    "Есть например Cosine Similarity.\n",
    "\n",
    "![](https://miro.medium.com/max/433/0*WmZ-uk5VzfY7RiBt)\n",
    "\n",
    "Оно часто используется в противовес эвклидового расстояния, так как не имеет проблем с высокой размерностью данных. Мы можем его например использовать в задачах анализа текста, когда данные представлены количеством слов. Например, когда слово встречается в одном документе чаще над другим, это не обязательно означает, что один документ больше связан с этим словом. Может случиться так, что документы имеют неравномерную длину, а величина подсчета имеет меньшее значение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Synthetic Titanic Survival Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Read the description here: https://www.kaggle.com/c/tabular-playground-series-apr-2021/data. Download the dataset and place it in the *data/titanic/* folder in your working directory.\n",
    "You will use train.csv for model training and validation. The test set is used for model testing: once the model is trained, you can predict whether a passenger survived or not for each passenger in the test set, and submit the predictions: https://www.kaggle.com/c/tabular-playground-series-apr-2021/overview/evaluation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(PATH, 'titanic', 'train.csv')).set_index('PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** How many females and males are there in the dataset? What about the survived passengers? Is there any relationship between the gender and the survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Females', round(data.groupby(by='Sex').count().iloc[0, 1] / data.shape[0] * 100, 2), '%')\n",
    "print('Males', round(data.groupby(by='Sex').count().iloc[1, 1] / data.shape[0] * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stats_survived = data.groupby(by='Survived').Survived.count()\n",
    "stats_survived = pd.DataFrame([['Погибло', stats_survived[0], stats_survived[0] / 100_000 * 100],\n",
    "                               ['Выжило', stats_survived[1], stats_survived[1] / 100_000 * 100],\n",
    "                               ['Итого', 100_000, 100]], columns=['', 'Количество', '%'])\n",
    "stats_survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.title('Данные о погибших и выживших пассажиров')\n",
    "sns.barplot(x=stats_survived.iloc[:2, 0], y=stats_survived.iloc[:2, 1], palette=[\"r\", \"#2ecc71\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.title('Данные о погибших,выживших мужчин и женщин')\n",
    "sns.countplot(data=data, x='Survived', hue=\"Sex\", palette=[\"blue\", \"purple\"]).set(xlabel=\"ПОГИБЛО - ВЫЖИЛО\",\n",
    "                                                                                  ylabel=\"Количество\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fm = pd.get_dummies(data.Sex)\n",
    "newdata = pd.concat((fm, data), axis=1)\n",
    "newdata\n",
    "newdata = newdata.drop([\"male\"], axis=1)\n",
    "newdata = newdata.rename(columns={\"female\": \"Gender\"})\n",
    "newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_lst = ['Gender', 'Survived']\n",
    "corr = newdata[feature_lst].corr()\n",
    "print(\"Коэффициент Пирсона\", round(newdata[feature_lst].corr(method='pearson').iloc[1, 0], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Незначительная связь между Полом и Выживанием существует."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Plot age distribution of the passengers. What is the average and the median age of survived and deceased passengers? Do age distributions differ for survived and deceased passengers? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Age_Range'] = pd.cut(data.Age, [0, 1, 7, 16, 25, 35, 55, 100])\n",
    "data['Age_Range'] = data.Age_Range.astype('str')\n",
    "data['Age_Range'] = data.Age_Range.fillna('nan')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.title('Данные о погибших и выживших для возрастных групп')\n",
    "age_order = ['nan', '(0, 1]', '(1, 7]', '(7, 16]', '(16, 25]', '(25, 35]', '(35, 55]', '(55, 100]']\n",
    "sns.countplot(data=data, x='Age_Range', hue=\"Survived\", order=age_order, palette=[\"r\", \"#2ecc71\"]).set(\n",
    "    xlabel=\"Возрастные группы\", ylabel=\"Количество\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.groupby(by=['Survived']).Age.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.groupby(by=['Survived']).Age.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделяются возрастные группы в границах 16 лет до 35 лет, а также младенцы до 1 года - почти в 2 раза больше погибших пассажиров этих возрастов, чем выживших. Им менее повезло чем остальным, остальные же пассажиры (в возрасте от 1 года до 16 и старше 35 лет) имели по статистике почти равный шанс как спастись, так и погибнуть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 point)** Explore \"passenger class\" and \"embarked\" features. What class was \"the safest\"? Is there any relationship between the embarkation port and the survival? Provide the corresponding visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.Embarked.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Три уникальных порта. Данные указаны верно. C = Шербуре (Cherbourg), Q = Квинстаун (Queenstown), S = Саутгемптоне (Southampton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.Pclass.value_counts().index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Данные по Pclass содержат только значения 1,2,3 - что полностью совпадает с классами кают. Данные корректные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_data = data.groupby('Pclass').Survived.agg(['count', 'sum'])\n",
    "class_data['Survived_ratio'] = round(class_data['sum'] / class_data['count'] * 100)\n",
    "class_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Люди из высших классов имели больше шансов выжить. 58% людей из высшего класса, 53% людей из среднего класса и только 25% процентов людей из низшего класса выжили."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embarked_survival_data = data.groupby('Embarked').Survived.agg(['count', 'sum'])\n",
    "embarked_survival_data['survival_ratio'] = round(embarked_survival_data['sum'] / embarked_survival_data['count'] * 100)\n",
    "embarked_survival_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Пассажиры, что ступили на борт в порту Саутгемптоне имели шанс на выживание всего 32%. В других портах шанс погибнуть был меньше, шанса выжить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "На диаграмах ниже можно визуально наблюдать эти же сведения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "group = data.groupby(['Pclass', 'Survived'])\n",
    "pclass_survived = group.size().unstack()\n",
    "\n",
    "sns.heatmap(pclass_survived, annot=True, fmt=\"d\", cmap=sns.light_palette(\"#2ecc71\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "group = data.groupby(['Embarked', 'Survived'])\n",
    "pclass_survived = group.size().unstack()\n",
    "\n",
    "sns.heatmap(pclass_survived, annot=True, fmt=\"d\", cmap=sns.light_palette(\"#2ecc71\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "group = data.groupby(['Embarked', 'Sex'])\n",
    "pclass_survived = group.size().unstack()\n",
    "\n",
    "sns.heatmap(pclass_survived, annot=True, fmt=\"d\", cmap=sns.light_palette(\"#2ecc71\", as_cmap=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.Fare.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(data.Fare.median(), data.Fare.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Find the percentage of missing values for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Процент пропущенных значений:\\n', round(data.isnull().sum() / data.shape[0] * 100, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about the ways to handle these missing values for modelling and write your answer below. Which methods would you suggest? What are their advantages and disadvantages?\n",
    "\n",
    "Можно использовать dropna(), через loc. Например, работаем с возрастом и тарифом ( Age и Fare) и исследуем выживание , то код мог выглядеть так\n",
    "`data.loc[:, ['Survived', 'Fare', 'Age']].dropna()`\n",
    "Также можно попробовать заменить Null значения, на другие, подходящие по смыслу и ожиданиям. Делается это с помощью pandas.DataFrame.fillna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.5 points)** Prepare the features and train two models (KNN and Logistic Regression) to predict the survival. Compare the results. Use accuracy as a metric. Don't forget about cross-validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Подготовим данные\n",
    "\n",
    "План:\n",
    "- Атрибут Sex переделываем в Gender. 1 - жен. 0 - муж.\n",
    "- Embarked может быть Null. Не исключено, что такой объект может быть в тестовых данных, по которым нам предстоит дать прогноз, о том выжил или не выжил пассажир. Т.е. удалять такой объект нельзя. При исследовании мы видели, что больше всего село в порту S и погибших среди других портов тоже тут больше. Если это женщина, то скорее всего она выжила и села в порту S или Q, если мужчина, то погиб и вероятнее всего сел в порту S. Попробуем поставить всем пассажирам Embarked = S, если это мужчина, а если женщина - среднее между Q и S, там где значение Null.\n",
    "- Embarked C, Q, S переделаем в 1, 2, 3\n",
    "- 3% нулевых значений по Age. Меняем на ~значение медианы (Возраст 39 лет).\n",
    "- Нулевые Fare поменяем на ~значение медианы\n",
    "\n",
    "Остановимся пока на этом. Конечно можно усовершенствовать наши данные и дальше, но для этого нужно производить доп. исследования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prep_data(data: pd.DataFrame, return_y=True):\n",
    "    work_data = data.copy()\n",
    "\n",
    "    work_data.Sex = np.where(work_data.Sex == 'male', 0, 1)\n",
    "\n",
    "    work_data.Embarked = np.where((work_data.Embarked.isnull()) & (work_data.Sex == 0), 0.35, work_data.Embarked)\n",
    "    work_data.Embarked = np.where((work_data.Embarked.isnull()) & (work_data.Sex == 1), 0.525, work_data.Embarked)\n",
    "    work_data.Embarked = work_data.Embarked.replace(['S', 'C', 'Q', ], [0.3, 0.60, 0.75])\n",
    "\n",
    "    work_data.Age = work_data.Age.fillna(10.0)\n",
    "\n",
    "    work_data.Fare = work_data.Fare.fillna(24.5)\n",
    "\n",
    "    work_data['Family'] = work_data.SibSp + work_data.Parch\n",
    "    work_data['Alone'] = np.where(work_data['Family'] == 0, 1, 0)\n",
    "\n",
    "    X = work_data.loc[:, ['Sex', 'Pclass', 'Embarked', 'Age', 'Fare', 'Family', 'Alone']]\n",
    "\n",
    "    if return_y:\n",
    "        Y = work_data.Survived\n",
    "        return X, Y\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = prep_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Процент пропущенных значений:\\n', round(X.isnull().sum() / X.shape[0] * 100, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Создадим функцию, которая будет выводить текстовые сведения по [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def describe_conf_matrix_results(confusion_matrix_results):\n",
    "    tn, fp, fn, tp = confusion_matrix_results.ravel()\n",
    "    print('Позитив - выжил, негатив - погиб')\n",
    "    display(pd.DataFrame((\n",
    "        ('выжил', 'выжил', 'TP', tp),\n",
    "        ('погиб', 'выжил', 'FN', fn),\n",
    "        ('погиб', 'погиб', 'TN', tn),\n",
    "        ('выжил', 'погиб', 'FP', fp),\n",
    "    ), columns=['УТВЕРЖДАЕМ', 'РЕАЛЬНОСТЬ', 'МЕТКА', 'КОЛИЧЕСТВО РАЗ']))\n",
    "    print('Значение Accuracy составляет', (tp + tn) / (tp + tn + fp + fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Создадим функцию, которая будет выводить визуальные данные по Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def confusion_matrix_display(confusion_matrix_results):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_results)\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Создадим две модели KNN и Logistic Regression. Посмотрим на результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Input_norm = [\n",
    "    ('standardScaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())]\n",
    "pipeKnn = Pipeline(Input_norm)\n",
    "\n",
    "Input_norm = [\n",
    "    ('standardScaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression())]\n",
    "pipeLogreg = Pipeline(Input_norm)\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=77, shuffle=True)\n",
    "\n",
    "pipes = [\n",
    "    pipeKnn,\n",
    "    pipeLogreg,\n",
    "]\n",
    "\n",
    "for pipe in pipes:\n",
    "    scores = cross_val_score(pipe, X, Y, cv=kfold)\n",
    "    display(scores)\n",
    "    y_pred = cross_val_predict(pipe, X, Y, cv=kfold)\n",
    "    conf_mat = confusion_matrix(Y, y_pred)\n",
    "    confusion_matrix_display(conf_mat)\n",
    "    describe_conf_matrix_results(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 + X points)** Try more feature engineering and hyperparameter tuning to improve the results. You may use either KNN or Logistic Regression (or both)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Select the best model, load the test set and make the predictions. Submit them to kaggle and see the results :)\n",
    "\n",
    "**Note**. X points will depend on your kaggle public leaderboard score.\n",
    "$$ f(score) = 1.0, \\ \\ 0.79 \\leq score < 0.80,$$\n",
    "$$ f(score) = 2.5, \\ \\ 0.80 \\leq score < 0.81,$$ \n",
    "$$ f(score) = 4.0, \\ \\ 0.81 \\leq score $$ \n",
    "Your code should generate the output submitted to kaggle. Fix random seeds to make the results reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Модель LogisticRegression ведёт себя лучше, чем KNN. LogisticRegression определяет больше корректных выживших и меньше(на ~2000) ошибается, когда прогнозирует погибшего. Выбираем её. Посмотрим на результаты при разных параметрах LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for penalty in ['l1', 'l2']:\n",
    "    for c in [1.0, 0.5]:\n",
    "        print(penalty, c)\n",
    "        Input_norm = [\n",
    "            ('standardScaler', StandardScaler()),\n",
    "            ('logreg', LogisticRegression(penalty=penalty, C=c, solver='liblinear', random_state=kfold.random_state))]\n",
    "        pipeLogreg = Pipeline(Input_norm)\n",
    "        y_pred = cross_val_predict(pipeLogreg, X, Y, cv=kfold)\n",
    "        conf_mat = confusion_matrix(Y, y_pred)\n",
    "        describe_conf_matrix_results(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Из вышеуказанных результатов L2 и С=0.5 выглядят лучше остальных. Посмотрим на max_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for with_std in [True, False]:\n",
    "    print(with_std)\n",
    "    Input_norm = [\n",
    "        ('standardScaler', StandardScaler(with_std=True,with_mean=with_std)),\n",
    "        ('logreg', LogisticRegression(penalty='l2', C=1, solver='liblinear', random_state=kfold.random_state))]\n",
    "    pipeLogreg = Pipeline(Input_norm)\n",
    "    y_pred = cross_val_predict(pipeLogreg, X, Y, cv=kfold)\n",
    "    conf_mat = confusion_matrix(Y, y_pred)\n",
    "    describe_conf_matrix_results(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeLogreg.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(os.path.join(PATH, 'titanic', 'test.csv')).set_index('PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test = prep_data(test_data, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y_pred_test = pipeLogreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'Survived': Y_pred_test.astype('int64')}, index=test_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results.to_csv(os.path.join(PATH, 'titanic', 'submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}