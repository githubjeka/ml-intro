{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each task that is proposed to be completed as part of the homework has a declared \"price\" in points. The maximum possible amount is 10 points, and together with the bonus assignment - 12 points. It is not necessary to complete all the tasks, only a part can be done. Most of the points expect you to write working Python code; sometimes you will need to write comments - for example, to compare several approaches to solve the same problem. Also you can add more cells for your convenience if you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework focuses on clustering. We will work with images of handwritten digits, learn how to cluster them using two different methods (hierarchical clustering and the ùêæ-means algorithm), evaluate the quality of the partition and choose the optimal number of clusters, as well as visualize intermediate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data\n",
    "The data we will be working with is available in the scikit-learn library (`sklearn` module) in the `datasets` submodule via the `load_digits` function. The data contains 1,797 observations, each of which is 8√ó8 pixel image of a handwritten digit from 0 to 9. This is about the same amount of each digit (about 180).\n",
    "\n",
    "For convenience, every image expands to a 64 (8√ó8) row, so entire numpy array is 1797√ó64. The color intensity in each pixel is encoded with an integer from 0 to 16.\n",
    "\n",
    "In addition to images, their labels are also known. In this task, we will assume that the labels (as well as their amount) are unknown and try to group the data in such a way that the resulting clusters 'better match' the original ones. Possible options for determining the 'better match' are presented later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.25 points)** Load the images into `X` variable, and their labels into `y` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X, y = datasets.load_digits(n_class=10, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape == (1797, 64)\n",
    "assert y.shape == (1797,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Visualize the first 10 images.\n",
    "\n",
    "- Arrange images on a grid rather than in a row. You may need the `subplot` and `imshow` functions from the `pyplot` module in the `matplotlib` library.\n",
    "- You will also need to reshape the images to 8√ó8.\n",
    "- Remove ticks and labels from both axes. The `xticks` and `yticks` functions or the `tick_params` function from `pyplot` can help you with this.\n",
    "- Make the output good sized with the `figure` function from `pyplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x360 with 10 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "_, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 5))\n",
    "for ax, image, label in zip(axes.flatten(), X, y):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image.reshape((8, 8)), cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering and quality evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2 points)** Implement the the KMeans algorithm. Use objective function $L = \\sum_{i=1}^{n}|x_{i}-Z_{A(x_{i})}|^{2}$, where $Z_{A(x_{i})}$ is the center of the cluster corresponding to $x_{i}$ object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import NotFittedError\n",
    "from numpy.random import RandomState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomKMeans:\n",
    "    def __init__(self, n_clusters=2, max_iter=30, n_init=10, random_state=42):\n",
    "        '''K-Means clustering.\n",
    "        \n",
    "        Args:\n",
    "            n_clusters: int, default=2\n",
    "                The number of clusters to be formed is also \n",
    "                the number of centroids to generate. \n",
    "            max_iter: int, default=300\n",
    "                Maximum number of iterations of the k-means algorithm for a\n",
    "                single run.\n",
    "            n_init: int, default=10\n",
    "                Number of time the k-means algorithm will be run with different\n",
    "                centroid seeds. The final results will be the best output of\n",
    "                n_init consecutive runs in terms of objective function.\n",
    "            random_state: int, default=42\n",
    "                Random state.\n",
    "        '''\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_init = n_init\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = RandomState(seed=random_state)\n",
    "        \n",
    "    def calculate_distances_to_centroids(self, X, cluster_centers):\n",
    "        \"\"\"\n",
    "        Returns (n, c) matrix where the element at position (i, j) \n",
    "        is the distance from i-th object to j-th centroid.\"\"\"\n",
    "        # <your code>\n",
    "        pass\n",
    "    \n",
    "    def update_centroids(self, X, nearest_clusters):\n",
    "        \"\"\"\n",
    "        Returns numpy array of shape (n_clusters, n_features) - \n",
    "        new clusters that are found by averaging objects belonging \n",
    "        to the corresponding cluster.\"\"\"\n",
    "        # <your code>\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit the model.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        assert X.shape[0] >= self.n_clusters\n",
    "        # <your code>\n",
    "        \n",
    "        return self\n",
    "            \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes.\n",
    "        \n",
    "        Args:\n",
    "            X: numpy array of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            y: numpy array of shape (n_samples,)\n",
    "                Vector containing predicted cluster labels.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'cluster_centers_'):\n",
    "            # <your code>\n",
    "            pass\n",
    "        else: \n",
    "            raise NotFittedError(\"CustomKMeans instance is not fitted yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 points)** Get the `X` array partition into 10 clusters. Visualize the centers of clusters.\n",
    "- We will assume that the center of the cluster is average value of all observations belonging to the cluster.\n",
    "- The cluster centers should have the same shape as our observations (64). So you have to average the points across the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_kmeans_labels = ...\n",
    "assert custor_kmeans_labels.shape == (1797,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Experiment with `max_iter` and `n_init` parameters. Look at the range of values of the objective function, it's best values, at what parameters and how often they are achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use two popular algorithms: hierarchical clustering and $K$-means clustering. These and other algorithms are available in the `scikit-learn` module in the `cluster` submodule. Hierarchical clustering is called `AgglomerativeClustering`, and the $K$-means method is called `KMeans`.\n",
    "\n",
    "**(0.5 points)** Use each of the two methods: hierarchical clustering and KMeans. Get the `X` array partition into 10 clusters.\n",
    "\n",
    "- Note that `AgglomerativeClustering` does not have a `predict` method, so you can either use the `fit_predict` method or use the `fit` method and then look at the `labels_` attribute of the class instance.\n",
    "- Kmeans performs multiple runs (default 10) with random centers and then returns the best partition in terms of average distance within the clusters. You can increase the number of runs to improve the quality of predictions in the `i_init` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_labels = ...\n",
    "kmeans_labels = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hierarchical_labels.shape == (1797,)\n",
    "assert kmeans_labels.shape == (1797,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Visualize the centers of clusters obtained by both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a situation where the true number of classes is unknown, we can select it by maximazing some metric.\n",
    "\n",
    "When we can set some distance function between our observations, we can consider the `silhouette` distance as a function of measuring the quality of the clustering. Let's show how it is calculated:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ ‚Äì set of observations, $M \\subset X$ ‚Äì one of the clusters obtained as a result of clustering process, $\\rho$ ‚Äì some metric on $X$. Let's choose one observation $x \\in M$. Denote $a(x)$ as the average distance from $x$ to  $x'$ points from the same cluster:\n",
    "$$\n",
    "a(x) = \\frac{1}{|M| - 1} \\sum_{x' \\in M,\\, x' \\ne x} \\rho(x,\\, x')\n",
    "$$\n",
    "\n",
    "Denote $b(x)$ as minimun of average distances from $x$ to $x''$ from some other cluster $N$:\n",
    "$$\n",
    "b(x) = \\min_{N \\ne M} \\frac{1}{|N|} \\sum_{x'' \\in N} \\rho(x,\\, x'')\n",
    "$$\n",
    "\n",
    "The silhouette is difference between a(x) and b(x), normalized to $[-1, \\, 1]$ and averaged over all observations:\n",
    "$$\n",
    "\\frac{1}{|X|} \\sum_{x \\in X} \\frac{b(x) - a(x)}{\\max(a(x),\\, b(x))}\n",
    "$$\n",
    "\n",
    "The implementation of this metric in the `scikit-learn` is the `silhouette_score` function from the `metrics` submidule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**(0.75 point)** For each $K$ between 2 and 20 inclusive, partition of the array $X$ into $K$ clusters using both methods. Calculate the silhouette score and visualize it for both methods on the same plot ($K$ on the $x$ axis and silhouette score on the $y$ axis). Sign the axes and make a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we know the true clustering labels, the clustering result can be compared to them using measures such as `homogeneity`, `completeness` and their harmonic mean - $V$-score. The definitions of these quantities are rather bulky and are based on the [entropy of the probability distribution](https://ru.wikipedia.org/wiki/–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è_—ç–Ω—Ç—Ä–æ–ø–∏—è). Details are given in [this article](http://aclweb.org/anthology/D/D07/D07-1043.pdf). In practice, it's enough to know that `homogeneity`, `completeness` and $V$-score are in the range from 0 and 1, and the more, the better.\n",
    "\n",
    "Since we know what digit each image is (`y` array), we can compare the clustering results to it using the measures listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Repeat the previous task using $V$-measure instead of silhouette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature space dimensionality reduction\n",
    "\n",
    "In some cases, especially when there are a large number of features, when not all of them are informative, and some of them are correlated, it can be useful to reduce the dimension of the feature space. This mean that instead of $d$ original features, we will go to $d'\\ll d$ new ones. And if earlier our data were presented in the form of an $n√ód$ matrix, then it will presented as a $n√ód'$.\n",
    "\n",
    "There are two popular dimensionality reduction approaches:\n",
    "- select new features from existing features;\n",
    "- extract the new features by transforming old ones, for example, by making $d'$ different linear combinations of columns of an $n√ód$ matrix.\n",
    "\n",
    "One widely used dimensionality reduction technique is the Singular Value Decomposition (SVD). This method allows you to construct any number $d'\\leq d$ of new features in such a way that they are the most informative (in some sense).\n",
    "\n",
    "The `scikit-learn` module has several implementations of singular value decomposition. We will use the `TruncatedSVD` class from the `decomposition` submodule.\n",
    "\n",
    "**Note:** The singular value decomposition of the matrix $M$ is usually written as $M=U \\Sigma V^{*}$. `TruncatedSVD`, in turn, returns only the $d'$ first columns of the matrix $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.75 point)** Perform a singular value decomposition of the $X$ matrix, leaving 2, 5, 10, 20 features. In each case, perform hierarchical clustering and $K$-Means clustering (take the number of clusters equal to 10). Calculate the silhouette and $V$-score and compare them to corresponding values obtained from the original data.\n",
    "\n",
    "**Note**: It is not valid to compare the silhouette calculated with different metrics. Even if we use the same metric function when calculating the distance between points in the data, after applying dimensionality reduction or other data transformations, we will (not always) get different silhouette scores. Therefore, after training the clustering algorithm, to compare the result of clustering, you need to calculate the silhouette on the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular dimensionality reduction approach that is useful for working with images is t-distributed stochastic neighbor embeddings, abbreviated `tSNE`. Unlike singular value decomposition, this it is non-linear transformation. It's main idea is to map points from a space of dimension `d` to another space of dimension 2 or 3 in such a way that the distances between points are mostly preserved. Mathematical details can be found, for example, [here](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding).\n",
    "\n",
    "The implementation of `tSNE` in the `scikit-learn` library is the `TSNE` class in the `manifold` submodule.\n",
    "\n",
    "**Note:** In recent years [UMAP](https://github.com/lmcinnes/umap) is often used istead of `tSNE`. It is a faster algorithm with similar properties. We don't ask you to use `UMAP` because it requires you to install another dependency, the `umap-learn` library. Those who wish can perform the following task using `UMAP`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Perform a tSNE-transform of the `X` matrix, leaving 2 features. Visualize the obtained data in the form of a scatter plot form: the first feature on the horizontal axis, and the second one the vertical axis. Color the points according to the digits they belong to.\n",
    "\n",
    "- The `c` parameter in the plt.scatter function is responsible for the color of the points. Pass the true labels to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** From the data transformed using the tSNE, perform hierarchical clustering and $K$-means clustering (take the number of clusters equal to 10). Calculate the silhouette and the $V$-score and compare them to corresponding values obtained from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.25 points)** Choose the best partition (in terms of silhouette or $V$-score) and visualize the centers of clusters with images. Did you managed to make each digit correspond to one center of the cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results and bonus part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write in free form what conclusions you made after completing this assignment. Answer the following questions:\n",
    "\n",
    "**(0.5 points)** Which algorithm gives more meaningful results - hierarchical clustering or $K$- means clustering. Does it depend on the algorithm settings or on the quality evaluation method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** Imagine the situation where after hierarchical clustering, you need to cluster new data in the same way without retraining the model. Suggest a method how you will do it and how you will measure the quality of clustering of new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**(0.5 points)** Does dimensionality reduction improve clustering results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 points)** How to evaluate the quality of dimensional reduction? Suggest at least 2 options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Bonus 2 points)** Load the [MNIST Handwritten Digits](http://yann.lecun.com/exdb/mnist) dataset. You can also do it with `scikit-learn` as explained [here](https://stackoverflow.com/a/60450028). Explore the data and try to cluster it using different approaches. Compare results of these approaches using the silhouette and the $V$-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}