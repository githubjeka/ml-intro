{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectFromModel, RFECV, SequentialFeatureSelector\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.datasets import make_classification, load_wine, load_breast_cancer, load_diabetes, load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(x, y, auto_scaled=True, title=None, clusters=None):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.scatter(x, y, c=clusters, cmap='bwr')\n",
    "    \n",
    "    if not auto_scaled:\n",
    "        plt.axis('square')\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def return_X_y(data, target_column):\n",
    "    return data.drop(target_column, axis=1), data[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_sklearn = load_wine(as_frame=True)\n",
    "wine_data, wine_labels = wine_sklearn['data'], wine_sklearn['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise  1 - Scaling (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform standardization for wine dataset (`wine_data`) using only basic python, numpy and pandas (without using `StandardScaler` and sklearn at all). Implementation of function (or class) that can get dataset as input and return standardized dataset as output is preferrable, but not necessary.\n",
    "\n",
    "Compare you results (output) with `StandardScaler`.\n",
    "\n",
    "**NOTE:**\n",
    "\n",
    "- 1 point for functional version, 2 points for implementing scaling as sklearn pipeline compartible class. \n",
    "- Maximum for the exercise is 2 points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple version (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 2 (2788556220.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Input \u001B[1;32mIn [5]\u001B[1;36m\u001B[0m\n\u001B[1;33m    # your code here\u001B[0m\n\u001B[1;37m                    ^\u001B[0m\n\u001B[1;31mIndentationError\u001B[0m\u001B[1;31m:\u001B[0m expected an indented block after function definition on line 2\n"
     ]
    }
   ],
   "source": [
    "# 1 point\n",
    "def scale(X):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(np.array(scale(wine_data)), StandardScaler().fit_transform(wine_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Version (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 points\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class CustomScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, copy=True, with_mean=True, with_std=True):\n",
    "        # your code here\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # your code here\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, copy=None):\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(CustomScaler().fit_transform(wine_data), StandardScaler().fit_transform(wine_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise  2 - Visualization (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted earlier, standardization/normalization of data can be crucial for some distance-based ML methods.\n",
    "\n",
    "Letâ€™s generate some toy example of unnormalized data and visualize the importance of this process once more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_0 = np.random.randn(1000) * 10   \n",
    "feature_1 = np.concatenate([np.random.randn(500), np.random.randn(500) + 5])\n",
    "data = np.column_stack([feature_0, feature_1])\n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(data[:, 0], data[:, 1], auto_scaled=True, title='Data (different axes units!)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** on the plot above axes are scaled differently and we can clearly see two potential *classes/clusters*. In fact `matplotlib` performed `autoscaling` (which is basically can be considered as `MinMaxScaling` of original data) just for better visualization purposes.\n",
    "\n",
    "Let's turn this feature off and visualize the original data on the plot with equally scaled axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(data[:, 0], data[:, 1], auto_scaled=False , title='Data (equal axes units!)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This picture is clearly less interpretable, but much closer to \"how distance-based algorithm see the original data\": separability of data is hardly noticable only because the variation (std) of x-feature is much bigger in absolute numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform `StandardScaling` and `MinMaxScaling` of original data; visualize results for each case (**use `plot_scatter` with `auto_scaled=False`**):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaling (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Bonus) K-means (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustrate the impact of scaling on basic distance-based clustering algorithm [K-means](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1) using `data` generated above.\n",
    "\n",
    "**NOTE:** basically, you don't need understanding K-means algorithm here, you just need to:\n",
    "\n",
    "1) run algorithm (with k=2, k - number of clusters/classes) on unscaled data    \n",
    "2) run algorithm (with k=2) on scaled data    \n",
    "3) plot results: highlight different clusters using different colors.\n",
    "\n",
    "You can use this [question](https://stats.stackexchange.com/questions/89809/is-it-important-to-scale-data-before-clustering/89813) as a hint, but I recommend you to plot results using `plot_scatter` with `equal_scaled=True`: it might help you to intuitively understand the reasons of such scaling impact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise  3 - Preprocessing Pipeline (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_train, wine_val, wine_labels_train, wine_labels_val = train_test_split(wine_data, wine_labels, \n",
    "                                                                            test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model (for example, `LogisticRegression(solver='liblinear', penalty='l1')` on raw `wine_train` data; then train same model after data scaling; then add feature selection (and train model again on scaled data). For each experiment all required preprocessing steps (if any) should be wrapped into sklearn pipeline.\n",
    "\n",
    "Measure `accuracy` of all 3 approaches on `wine_val` dataset. Describe and explain results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - manual PCA (5 points)\n",
    "The task is to solve PCA as an optimization problem, without explicitly doing eigen value decomposition.\n",
    "In the most general setting PCA is minimization of reconstruction error of a projection of given rank $q$\n",
    "\n",
    "$$\\min_{\\mu, \\lambda_1,\\ldots, \\lambda_n, \\mathbf{V}_q} \\sum_{i=1}^n ||x_i - \\mu - \\mathbf{V}_q \\lambda_i||^2$$\n",
    "\n",
    "With a number of steps that can be found here https://stats.stackexchange.com/a/10260 this task transforms to\n",
    " $$\\max_{u_i} \\sum_{i=1}^q u_i^T \\mathbf{S} u_i$$\n",
    " where $\\mathbf{S}$ is the sample covariance matrix (after standartization) and $u_1, \\ldots, u_q$ are the $q$ are orthonormal columns in $\\mathbf{V}_q$.\n",
    " Let us solve this optimization problem with `scipy.optimize` library.\n",
    " \n",
    " Additional 2 point are given for visualization of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_data, wine_labels = wine_sklearn['data'], wine_sklearn['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a covariance matrix of standartized data and assing it to S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code\n",
    "\n",
    "S = ## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your code is correct, the following assert should be Ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(np.linalg.norm(S), 5.787241159764733)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def objective(x):\n",
    "    # your code: write objective of the problem (don't forget that scipy does min while we need max)\n",
    "\n",
    "def norm_constraint(x):\n",
    "    # your code: constaraint norm of x to be 1, function should return 0 if constraint holds\n",
    "\n",
    "con1 = {'type': 'eq', 'fun': norm_constraint}\n",
    "\n",
    "x0 = # your code: initial vector to start optimization\n",
    "\n",
    "sol = minimize(objective, \n",
    "               x0, \n",
    "               constraints = [con1]\n",
    "              )\n",
    "x0 = sol.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray! We have first vector! Let's do another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonality_constraint(x):\n",
    "    # your code: x should be orthogonal to x0, function should return 0 if constraint holds\n",
    "\n",
    "con2 = {'type': 'eq', 'fun': orthogonality_constraint}\n",
    "\n",
    "x1 = # your code: initial vector to start optimization\n",
    "\n",
    "\n",
    "sol = minimize(objective, \n",
    "               x1, \n",
    "               constraints = #your code\n",
    "              )\n",
    "\n",
    "x1 = sol.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your solution is correct, the following asserts should be Ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(x0@S@x0, 4.732436977583595)\n",
    "assert np.allclose(x1@S@x1, 2.5110809296451233)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the points after applying custom dimension reduction with 2 components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - Boruta (3 points)\n",
    "\n",
    "Let us classify handwritten digits 0, 1 and 2. \n",
    "To make task not so easy the images are binarized (no shadows of gray present) as it happens with xerocopied documents.\n",
    "\n",
    "Let us also find out to which parts of an image there's no need to look in order to clasify three digits of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(n_class=3, return_X_y=True, as_frame=True)\n",
    "X = (X>10).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,3,figsize=(10,4))\n",
    "for i in range(3):\n",
    "    ax[i].imshow(X.iloc[i].values.reshape(8,8))\n",
    "    ax[i].set_title(f\"This is digit {y[i]}.\")\n",
    "plt.suptitle(\"First three images.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test, let test size be 30% of the dataset and fix random state to 42:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = ## your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_val.shape[0] == 162\n",
    "assert y_val.sum() == 169"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a RandomForestClassifier with max_depth=13 and evaluate it's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=13)\n",
    "\n",
    "# your code here\n",
    "\n",
    "acc = # your code here\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc > 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use Boruta to find redundand pixels. If the package is not installed in your system, uncomment and run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "\n",
    "feat_selector = BorutaPy(RandomForestClassifier(max_depth=13), \n",
    "                         n_estimators='auto', \n",
    "                         verbose=0, \n",
    "                         max_iter=100,\n",
    "                         random_state=42)\n",
    "\n",
    "# your code here: do run the boruta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print redundant pixels as a mask. Does the result looks similar to mine (or to Among us chracters)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array(feat_selector.support_).reshape(8,8)\n",
    "plt.imshow(mask);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end let us redo  classification but only with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=13)\n",
    "\n",
    "# your code here\n",
    "\n",
    "acc = # your code here\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc > 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Materials & References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. General article about feature engineering and selection (main reference):\n",
    "https://github.com/Yorko/mlcourse.ai/blob/master/jupyter_english/topic06_features_regression/topic6_feature_engineering_feature_selection.ipynb\n",
    "\n",
    "2. Feature engineering/preprocessing, using scikit-learn API (great code examples, but really brief explanation):    \n",
    "https://scikit-learn.org/stable/modules/preprocessing\n",
    "\n",
    "3. Feature scaling/normalization:     \n",
    "https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35\n",
    "\n",
    "4. Log Transform/power transform:    \n",
    "https://medium.com/@kyawsawhtoon/log-transformation-purpose-and-interpretation-9444b4b049c9\n",
    "\n",
    "6. Missing values preprocessing using scikit-learn API (great code examples, great explanation):    \n",
    "https://scikit-learn.org/stable/modules/impute.html\n",
    "\n",
    "7. Feature selection scikit-learn API (great code examples, great explanation):   \n",
    "https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n",
    "8. Melbourne housing dataset source:    \n",
    "https://www.kaggle.com/anthonypino/melbourne-housing-market"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}